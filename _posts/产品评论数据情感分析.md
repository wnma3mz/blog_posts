---
title: 产品评论数据情感分析
date: 2018-08-18 22:28:12
tags: []
categories: [杂七杂八]
---

NLP处理问题的框图（知识点）。To Be Continue！

<!-- more -->

# 产品评论数据情感分析

## 数据获取
## 文本评论预处理
#### 编码问题（可选）
#### 文本去重
###### 基本解释
 大量重复且无实际意义的数据，需要删除
###### 算法
 编辑距离去重
 计算两条语料的编辑距离，与阈值（提前设定好的一个值）进行判断，小于阈值就进行去重
 优点：有效去除了接近重复或完全重复的数据
 缺点：错删语义相近的语料，两句话虽然相似但并不需要删除。由于阈值设定的原因可能会错删。
 Simhash算法去重
 k-shingle算法(用的不多)
 Minhash算法
###### 选用的方法
 最简单粗暴：完全重复的语料进行剔除。见`programmer_2`
#### 机械压缩去词
###### 基本解释
 经过上面的处理之后，还有这样一部分数据并没有去除。如"好的好的好的好的好的好的好的好的好的"# 我们称之为连续重复语料，最常见的较长的无意义语料。
###### 处理方法
 在某些情况下，重复词带有一定的情感倾向，如果直接进行处理，会影响情感倾向的判断。
 所以这里需要对语料进行机械压缩去词处理，去除一些连续累赘的表达（不影响情感）。比如上面的句子，压缩为"好的"
 一般情况下，这种词汇常出现在开头或者结尾。语料中间的重复词汇可能具有实际意义比如"滔滔不绝"中的滔滔是具有实际意义的。因此只对开头与结尾的连续重复词进行操作。
###### 设定规则
 考虑因素：词法结构、文字表达特点
 七个规则
#### 短句删除
###### 字数越少表达的意思就少，所以需要过滤掉过短的评论文本数据。
## 文本评论分词
#### 分词介绍
###### python库：jieba
###### 引入停用词
###### 特征处理
###### 中文分词方法
 基于规则的分词方法
 基于统计的分词方法
 基于语义的分词方法
 基于理解的分词方法
## 模型构建
#### 情感倾向性模型
###### 训练生成词向量
 One-hot# Representation
 Distributed# Representation
 word2vec
###### 人工标注与映射
 snownlp
###### 训练模型
 栈式自编码网络
 基本介绍
#### 语义网络分析
###### 语义网络
 介绍
 步骤
 优势
#### 模型选择
###### LDA模型的主题分析
 模型介绍
 Bag Of Words（BOW，词袋模型）
 模型估计
 实现过程
 拓展：ILDA
###### VSM与TF-IDF
###### SVD与LSA
###### PLSA与LDA
#### 构建模型
###### SVM (RBF) + PCA
###### Subtopic
## 模型评价
#### 应用层次分析法
#### 模糊综合评价
