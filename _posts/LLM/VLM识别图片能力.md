---
title: VLM的识别图片能力
date: 2025-08-04 17:12:00
tags: [NLP, Attention]
categories: [Note]
mathjax: true
---

以模型视角对比不同模态输入的效果

<!-- more -->

## 推理层面

输入对比

|          | 文本                                                         | 图片                                                         |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 形态     | 3. 下列传统节日与习俗对应错误的是（ ）（2分）<br/>A. 中秋节：赏月、登高 <br/>B. 元宵节：吃元宵、猜灯谜<br/>C. 清明节：扫墓、踏青<br/>D. 端午节：吃粽子、赛龙舟 | ![](https://raw.githubusercontent.com/wnma3mz/blog_posts/master/imgs/VLM/029038fc4c286a111609944bf60f2e14_200_433_305_75_240.jpg) |
| token 数 | 69                                                           | size：240x75 <br> smart_resize：252*84 <br>84 * 252 // 28 //28=27 |


不同模态的输入给模型是不一样的信息。

对于一般 LLM 来说，文本模态有 69 个 token 需要理解并进行作答。而相同含义的输入，图片模态只有 27 个 token（小于 69 个 token）。

即，图片模态的 27 个 token 等效于文本模态的 69 个 token。

所以，直觉上，模型对于小图输入的问题，会更难以作答。即模型能够把 27 个 token “解压”回 69 个 token，才能与文本模态进行公平比较。

## 训练层面

基于上述发现，现在的训练相当于是基于一个压缩程度较高的数据进行学习的（增加了训练难度），推理的时候相当于降低了难度。

所以，如果在训练的时候把图片缩小 1/2，是不是进一步加剧训练难度，更大程度“强化”模型学习知识。

